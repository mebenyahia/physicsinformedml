{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accfdf80",
   "metadata": {},
   "source": [
    "# Pure Neural Network vs Physics Informed Neural Network\n",
    "\n",
    "Welcome back! In this notebook we compare two learning strategies for the mass–spring dataset created earlier.\n",
    "\n",
    "- A **baseline neural network** that only tries to match the noisy data points.\n",
    "- A **Physics Informed Neural Network (PINN)** that matches the data *and* respects the governing differential equation.\n",
    "\n",
    "Our goal is to see how the extra physics guidance helps the model produce better predictions, especially when we only supply a few noisy measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5926f",
   "metadata": {},
   "source": [
    "## 1. What are these models?\n",
    "\n",
    "Before coding, here is a quick reminder in plain language:\n",
    "\n",
    "- A **standard neural network** is a flexible function built from layers of simple mathematical operations. During training we show it input–output pairs (in our case time and displacement) and adjust its weights to reduce the prediction error.\n",
    "- A **Physics Informed Neural Network (PINN)** uses the same layers, but it also gets told about the physical law. Besides fitting the data, it tries to make the differential equation hold true at many time points. We penalize it whenever the equation is violated.\n",
    "\n",
    "By training both models side by side we can visualize how physics knowledge improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbb944",
   "metadata": {},
   "source": [
    "## 2. Imports and helper functions\n",
    "\n",
    "We import PyTorch for building the neural networks, NumPy for data handling, Matplotlib for plotting, and SciPy in case we need to regenerate the dataset from scratch. We also add the project root to `sys.path` so that we can reuse the `TimeMLP` model defined in `src/models.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    '''Locate the project directory regardless of where the notebook is executed from.'''\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / 'figures').exists() and (candidate / 'src').exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError('Could not locate the project root. Run the notebook from inside the repository folder.')\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "src_path = PROJECT_ROOT / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "from src.models import build_baseline_model, build_pinn_model, ModelConfig\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab3019",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Regenerate the dataset inside this notebook\n",
    "\n",
    "To keep everything lightweight we rebuild the synthetic dataset right here using the same helper logic as the first notebook. This avoids saving `.npz` files and makes it easy to tweak the parameters on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = 1.0\n",
    "c = 0.1\n",
    "k = 1.0\n",
    "\n",
    "def generate_dataset(\n",
    "    m_value: float = m,\n",
    "    c_value: float = c,\n",
    "    k_value: float = k,\n",
    "    noise_level: float = 0.02,\n",
    "    t_start: float = 0.0,\n",
    "    t_end: float = 10.0,\n",
    "    num_points: int = 1000,\n",
    "    seed: int | None = 0,\n",
    "):\n",
    "    \"\"\"Return clean and noisy displacement data for the mass-spring system.\"\"\"\n",
    "    time_eval = np.linspace(t_start, t_end, num_points)\n",
    "\n",
    "    def system(t, y):\n",
    "        x, v = y\n",
    "        dxdt = v\n",
    "        dvdt = -(c_value / m_value) * v - (k_value / m_value) * x\n",
    "        return [dxdt, dvdt]\n",
    "\n",
    "    solution = solve_ivp(system, (t_start, t_end), [1.0, 0.0], t_eval=time_eval)\n",
    "    if not solution.success:\n",
    "        raise RuntimeError('ODE solver failed when regenerating data.')\n",
    "\n",
    "    x_true = solution.y[0]\n",
    "    velocity = solution.y[1]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    signal_amplitude = np.max(np.abs(x_true))\n",
    "    noise_std = noise_level * signal_amplitude\n",
    "    x_noisy = x_true + rng.normal(scale=noise_std, size=x_true.shape)\n",
    "\n",
    "    return time_eval, x_true, x_noisy, velocity\n",
    "\n",
    "\n",
    "time, x_clean, x_noisy, velocity = generate_dataset()\n",
    "\n",
    "# Select training points in the range [0, 5] seconds\n",
    "train_mask = time <= 5.0\n",
    "train_time = time[train_mask]\n",
    "train_indices = np.linspace(0, train_time.size - 1, 40, dtype=int)\n",
    "train_time_subset = train_time[train_indices]\n",
    "train_observations = x_noisy[train_mask][train_indices]\n",
    "\n",
    "# Full test set\n",
    "test_time = time\n",
    "x_test_true = x_clean\n",
    "\n",
    "print(f'Training points: {train_time_subset.shape[0]} | Test points: {test_time.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a018f9",
   "metadata": {},
   "source": [
    "### Prepare tensors for PyTorch\n",
    "\n",
    "We convert the NumPy arrays to PyTorch tensors with shape `(N, 1)` so that they play nicely with our multilayer perceptron. Working with column vectors keeps the math clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4800abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(array: np.ndarray) -> torch.Tensor:\n",
    "    return torch.tensor(array, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "t_train = to_tensor(train_time_subset)\n",
    "x_train = to_tensor(train_observations)\n",
    "\n",
    "t_test = to_tensor(test_time)\n",
    "x_test = to_tensor(x_test_true)\n",
    "\n",
    "print('Training tensor shape:', t_train.shape)\n",
    "print('Test tensor shape:', t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00497c51",
   "metadata": {},
   "source": [
    "## 4. Baseline neural network training\n",
    "\n",
    "We use the `TimeMLP` defined in `src/models.py`. The network is small and trains quickly on a CPU. We minimize the mean squared error between the predictions and the noisy training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(hidden_layers=(64, 64), activation='tanh')\n",
    "baseline_model = build_baseline_model(config).to(device)\n",
    "\n",
    "optimizer = Adam(baseline_model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# DataLoader helps with batching even though the dataset is small\n",
    "train_dataset = TensorDataset(t_train, x_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "num_epochs = 1000\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_t, batch_x in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = baseline_model(batch_t)\n",
    "        loss = criterion(predictions, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_t.size(0)\n",
    "    epoch_loss /= len(train_dataset)\n",
    "    loss_history.append(epoch_loss)\n",
    "    if epoch % 200 == 0 or epoch == 1:\n",
    "        print(f'Epoch {epoch:4d} | Training MSE: {epoch_loss:.6f}')\n",
    "\n",
    "baseline_predictions = baseline_model(t_test).detach().cpu().numpy().flatten()\n",
    "baseline_mse = float(torch.mean((baseline_model(t_test) - x_test) ** 2).item())\n",
    "print(f'Baseline test MSE: {baseline_mse:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e0b17",
   "metadata": {},
   "source": [
    "### Visualize the baseline fit\n",
    "\n",
    "We plot the baseline network prediction against the true curve and highlight the training points. This reveals how the model behaves outside the region where it saw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b568af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_time, x_test_true, label='True solution', linewidth=2)\n",
    "ax.plot(test_time, baseline_predictions, label='Baseline NN prediction', linestyle='--')\n",
    "ax.scatter(train_time_subset, train_observations, label='Training data (noisy)', color='black', s=40, zorder=5)\n",
    "ax.axvspan(0, 5, color='gray', alpha=0.1, label='Training region')\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Displacement [m]')\n",
    "ax.set_title('Baseline neural network vs. true motion')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(PROJECT_ROOT / 'figures' / 'baseline_prediction.svg', dpi=150, format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51feea2",
   "metadata": {},
   "source": [
    "## 5. Train the Physics Informed Neural Network\n",
    "\n",
    "The PINN uses the same architecture but receives additional feedback from the differential equation. We evaluate the residual of the ODE at many time points and penalize large deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_model = build_pinn_model(config).to(device)\n",
    "optimizer_pinn = Adam(pinn_model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lambda_data = 1.0\n",
    "lambda_phys = 1.0\n",
    "\n",
    "# Physics points cover the full time range to encourage good behavior everywhere\n",
    "t_physics = torch.linspace(0.0, 10.0, steps=200, device=device).view(-1, 1)\n",
    "\n",
    "def physics_residual(model: nn.Module, t: torch.Tensor) -> torch.Tensor:\n",
    "    t = t.clone().detach().requires_grad_(True)\n",
    "    x_pred = model(t)\n",
    "    dx_dt = torch.autograd.grad(x_pred, t, grad_outputs=torch.ones_like(x_pred), create_graph=True)[0]\n",
    "    d2x_dt2 = torch.autograd.grad(dx_dt, t, grad_outputs=torch.ones_like(dx_dt), create_graph=True)[0]\n",
    "    residual = m * d2x_dt2 + c * dx_dt + k * x_pred\n",
    "    return residual\n",
    "\n",
    "num_epochs_pinn = 1500\n",
    "\n",
    "for epoch in range(1, num_epochs_pinn + 1):\n",
    "    optimizer_pinn.zero_grad()\n",
    "    pred_data = pinn_model(t_train)\n",
    "    data_loss = criterion(pred_data, x_train)\n",
    "\n",
    "    residual = physics_residual(pinn_model, t_physics)\n",
    "    physics_loss = torch.mean(residual ** 2)\n",
    "\n",
    "    loss = lambda_data * data_loss + lambda_phys * physics_loss\n",
    "    loss.backward()\n",
    "    optimizer_pinn.step()\n",
    "\n",
    "    if epoch % 300 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f'Epoch {epoch:4d} | Total loss: {loss.item():.6f} '\n",
    "            f'| Data: {data_loss.item():.6f} | Physics: {physics_loss.item():.6f}'\n",
    "        )\n",
    "\n",
    "pinn_predictions = pinn_model(t_test).detach().cpu().numpy().flatten()\n",
    "pinn_mse = float(torch.mean((pinn_model(t_test) - x_test) ** 2).item())\n",
    "print(f'PINN test MSE: {pinn_mse:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f168eb",
   "metadata": {},
   "source": [
    "### Compare all curves together\n",
    "\n",
    "The next plot overlays the true motion, the baseline network, and the PINN. We again highlight where training data were available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_time, x_test_true, label='True solution', linewidth=2)\n",
    "ax.plot(test_time, baseline_predictions, label='Baseline NN', linestyle='--')\n",
    "ax.plot(test_time, pinn_predictions, label='PINN', linestyle='-')\n",
    "ax.scatter(train_time_subset, train_observations, label='Training data (noisy)', color='black', s=40, zorder=5)\n",
    "ax.axvspan(0, 5, color='gray', alpha=0.1, label='Training region')\n",
    "ax.set_xlabel('Time [s]')\n",
    "ax.set_ylabel('Displacement [m]')\n",
    "ax.set_title('Baseline NN vs PINN on the mass–spring system')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(PROJECT_ROOT / 'figures' / 'model_comparison.svg', dpi=150, format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723a0a5",
   "metadata": {},
   "source": [
    "## 6. Quantitative comparison\n",
    "\n",
    "To summarize the experiment we compute the mean squared error (MSE) on the full test set for both models and present it in a small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    ('Baseline NN', baseline_mse),\n",
    "    ('PINN', pinn_mse),\n",
    "]\n",
    "\n",
    "print('Model           | Test MSE')\n",
    "print('---------------------------')\n",
    "for name, mse in results:\n",
    "    print(f'{name:<15} | {mse:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a7070",
   "metadata": {},
   "source": [
    "The PINN achieves a lower error because the physics loss keeps it aligned with the true dynamics even outside the training window. The baseline network, having no knowledge of the differential equation, drifts away once it leaves the area with data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4c284",
   "metadata": {},
   "source": [
    "## 7. Takeaways\n",
    "\n",
    "- Physics guidance acts like a smart regularizer: the PINN stays close to the real motion even with noisy, sparse data.\n",
    "- The baseline neural network can memorize the training points but struggles to extrapolate.\n",
    "- Both models share the same architecture; the difference lies in the training objective.\n",
    "- When you know the governing equation, incorporating it into the loss can dramatically boost reliability.\n",
    "\n",
    "Feel free to tweak the parameters, the number of training points, or the loss weights to see how the behavior changes!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
